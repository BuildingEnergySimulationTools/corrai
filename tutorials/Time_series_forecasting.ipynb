{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50880702abcbdcdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:48.182684Z",
     "start_time": "2024-01-02T08:48:48.120459200Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:49.477364300Z",
     "start_time": "2024-01-02T08:48:48.180624800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from corrai.measure import MeasuredDats\n",
    "from copy import deepcopy\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926bd05022e47ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968da07551c7346",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This tutorial guides you through the process of training a Machine Learning (ML) model in order to predict at short term (~6h), the electricity consumption of a tertiary building office equipments. \n",
    "\n",
    "The tutorial will follow the steps:\n",
    "- Load the raw data provided by the Building Energy Monitoring System (BEMS)\n",
    "- Use the Corrai object <code>MeasureDats</code> to pre-process, clean, and visualize the data\n",
    "- Train several ML model, select the most appropriate\n",
    "- Tune the selected model hyper parameters (TODO)\n",
    "- Assess and discuss the model performance\n",
    "- Wrap the model and the processing pipes for production (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccf80139e72cef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5124961fd4cd0d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data are coming from an electrical energy meter that returns an increasing index in kWh. The timestep is variable between 11 min and 22min. The electric equipments are office equipment, and probably lab equipments or server room chiller.\n",
    "\n",
    "First let's load the csv file containing a single column of data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccfee681486f5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:49.680624300Z",
     "start_time": "2024-01-02T08:48:49.478459400Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(Path(os.getcwd()) / \"resources/compteur_elec.csv\", index_col=0, sep=';')\n",
    "data_df.index = pd.to_datetime(data_df.index, format='mixed', utc=True)\n",
    "data_df = data_df.loc[\"2023-01-01 00:00:00\":, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de1bf4c94a359e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Use the <code>DataFrame</code> <code>info()</code> method to get some insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37043a29ba9a80d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:49.756862400Z",
     "start_time": "2024-01-02T08:48:49.681653100Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561683e4aee6756",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It looks that data are available from the 2022-12-09 to the 2023-11-22 (in order to get full days).\n",
    "It also looks that among the 22992 entries, none are Nan. This is good news\n",
    "\n",
    "We will use the <code>MeasureDats</code> object to get a visual representation and try to identify the gaps that will be annoying (greater than 3h, where a linear the interpolation may become \"dangerous\").\n",
    "\n",
    "_Note : this is a bit of a misuse of MeasureDats. For a proper introduction to the class, see the corresponding tutorial_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb6cb5bb38eb03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:49.845700200Z",
     "start_time": "2024-01-02T08:48:49.756862400Z"
    }
   },
   "outputs": [],
   "source": [
    "my_data = MeasuredDats(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbcb174ebb2620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:50.578409600Z",
     "start_time": "2024-01-02T08:48:49.831846800Z"
    }
   },
   "outputs": [],
   "source": [
    "my_data.plot_gaps(gaps_timestep=\"3H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420fd6d76fe2e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:50.707901700Z",
     "start_time": "2024-01-02T08:48:50.578905300Z"
    }
   },
   "outputs": [],
   "source": [
    "my_data.get_gaps_description(gaps_timedelta='3H')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c806ef98e043c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- A total of 5 gaps greater than 3h minutes were found.\n",
    "- The greater gap is 4 days long\n",
    "\n",
    "This is not bad for nearly a full year of data\n",
    "\n",
    "We can go a bit further and get the dates of the gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10cdeb67d6eda3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:50.986857900Z",
     "start_time": "2024-01-02T08:48:50.697663700Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.measure import find_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad604027bb2ff0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:51.215984400Z",
     "start_time": "2024-01-02T08:48:50.798461200Z"
    }
   },
   "outputs": [],
   "source": [
    "find_gaps(data_df, timestep='3h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0c25b4d69215",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given this results, we will probably trash the following periods:\n",
    "- __2023-04-07__ : 16h is to long for interpolation\n",
    "- __2023-05-19 to 2023-05-29__ : we choose to trash a full period. As we will see later, we are going to build sequences to train the ML models. Gaps will introduce errors, so we want to limit there number. It is better to throw away some good days\n",
    "- __2023-10-28 to 2023-10-30__ : the gap span over these two days\n",
    "\n",
    "Moreover, the previous figure showed an anomaly with the energy meter returning a negative value. This shall be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026ee238fde92c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "⚠️ __WARNING__ ⚠️\n",
    "Before going any further, we propose to split the dataset in 3 parts : __training, validation, testing__.\n",
    "We will not even look at the testing sample during the ML training process.\n",
    "This will avoid to introduce bias that could artificially improve our models performance.\n",
    "For example, when fitting transformers suche as <code>StandardScaler</code> on the full Dataset, we involuntary introduce knowledge on the data we later try to predict.\n",
    "The test sample will only be used to evaluate the model performance\n",
    "\n",
    "We proppose the following repartition : __80% training, 10% validation, 10% testing__\n",
    "\n",
    "Moreover we will not shuffle the dataset before splitting it. We are working with time series so the data are coherent chronologically. The training dataset will corespond to the beginning of the year, while the testing will correspond to the end.\n",
    "Although doing so, there might be a seasonal effect that it is not taken into account, it is better to have a coherent sequence and not random fragments of time series\n",
    "This could lead a major issue if the data had seasonal trend. But we will see later that it is not the case.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc037dee74ef1496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:51.722928400Z",
     "start_time": "2024-01-02T08:48:50.914663700Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = data_df.shape[0]\n",
    "train_raw_df = data_df.iloc[:int(data_size*0.8), :].copy()\n",
    "train_raw_df = train_raw_df.loc[:\"2023-05-01\", :]\n",
    "valid_raw_df = data_df.iloc[int(data_size*0.8):int(data_size*0.9), :].copy()\n",
    "tests_raw_df = data_df.iloc[int(data_size*0.9):, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bae2d36639264f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f82ffcaa66079",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The energy meter data returns a monotonously increasing time series.\n",
    "However we would like to have the electric energy rate [W] absorbed by the equipments.\n",
    "We propose the following transformations to process the time series :\n",
    "- remove negative values\n",
    "- convert the kWh to Joules\n",
    "- apply time gradient\n",
    "- Fill in the small gaps (<=3h) using linear interpolation\n",
    "- Apply Back filling and Front filling operation to make sure no Nan value are present at the beginning and at the and of the time series\n",
    "- Resample the timeseries to get a constant 15 min timestep\n",
    "- The resampling introduced new gaps, when the timestep was greater than 15min. Fill them using linear interpolation again.\n",
    "\n",
    "To easily generate the pipeline and visualise its effect, we use the <code>common_transformations</code> of the <code>MeasureDats</code>. We define a single transformation process called ... <code>process</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3652aac0490c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:51.799917200Z",
     "start_time": "2024-01-02T08:48:51.016960900Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.measure import Transformer, AGG_METHOD_MAP, AggMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b0b2a835f111d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:51.817336300Z",
     "start_time": "2024-01-02T08:48:51.112801300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_md = MeasuredDats(train_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b0224710490e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:51.835082300Z",
     "start_time": "2024-01-02T08:48:51.199407500Z"
    }
   },
   "outputs": [],
   "source": [
    "train_md.common_trans = {\n",
    "    \"Process\": [\n",
    "        [Transformer.DROP_THRESHOLD, {'lower': 0}],\n",
    "        [Transformer.APPLY_EXPRESSION, {\"expression\": \"X * 1000 * 3600\"}],\n",
    "        [Transformer.INTERPOLATE, {\"method\": \"linear\"}],        \n",
    "        [Transformer.TIME_GRADIENT, {}],\n",
    "        [Transformer.BFILL, {}],\n",
    "        [Transformer.FFILL, {}],\n",
    "        [Transformer.RESAMPLE, {\"rule\": \"15T\", \"method\": AGG_METHOD_MAP[AggMethod.MEAN]}],\n",
    "        [Transformer.INTERPOLATE, {\"method\": \"linear\"}],\n",
    "    ]\n",
    "}\n",
    "\n",
    "train_md.transformers_list = [\"Process\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf06afe4199216d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets see the effect of the pipe !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61686d231e0c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:52.038222300Z",
     "start_time": "2024-01-02T08:48:51.295621700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_md.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a527e903a2389b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It really looks good ! We can make the following observation :\n",
    "- There is a weekly pattern with fewer consumption during the weekends\n",
    "- There is a daily pattern, the energy rate starts to increase around 6 o clock and drops around 17h\n",
    "- For an unknown reason, there seem to be a pattern of period ~2h. It could be chiller, for a server local for exemple\n",
    "- It's not very clear, but the energy rate demand seem to increase slightly during the cold months\n",
    "- There seem to be a holiday period from the 25 December to the 2nd of january\n",
    "\n",
    "We can also see the gaps in the data. For example, if you try to zoom in on the second half of the month of May, you should see straight lines corresponding the linear interpolation. Lets remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cca49bfa1cc117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:52.077571800Z",
     "start_time": "2024-01-02T08:48:51.945134700Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_list_of_day(df, list_of_day):\n",
    "    \"\"\"\n",
    "    Drops rows from a pandas DataFrame based on a list of specific days.\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame from which rows will be dropped.\n",
    "    - list_of_day (list): A list of dates in string format ('YYYY-MM-DD') representing the days to be dropped.\n",
    "    \"\"\"\n",
    "    days_to_drop_dt = pd.to_datetime(list_of_day, errors='coerce')\n",
    "    return df[~df.index.strftime('%Y-%m-%d').isin(days_to_drop_dt.strftime('%Y-%m-%d'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03fc55118ea5571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:52.254280400Z",
     "start_time": "2024-01-02T08:48:52.037668900Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of full days to drop\n",
    "days_to_drop_train = [\n",
    "    \"2023-04-06\",\n",
    "    \"2023-04-07\",\n",
    "    \"2023-05-19\",\n",
    "    \"2023-05-20\",\n",
    "    \"2023-05-21\",\n",
    "    \"2023-05-22\",\n",
    "    \"2023-05-23\",\n",
    "    \"2023-05-24\",\n",
    "    \"2023-05-25\",\n",
    "    \"2023-05-26\",\n",
    "    \"2023-05-27\",\n",
    "    \"2023-05-28\",\n",
    "    \"2023-05-29\",\n",
    "]\n",
    "days_to_drop_test = [\n",
    "    \"2023-10-28\",\n",
    "    \"2023-10-29\",\n",
    "    \"2023-10-30\",\n",
    "]\n",
    "\n",
    "train_raw_df = drop_list_of_day(train_raw_df, days_to_drop_train).copy()\n",
    "tests_raw_df = drop_list_of_day(tests_raw_df, days_to_drop_test).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16408518892d657",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25ab46ab156763",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We could only use the time series, split it into sequences 18h for example, and try to predict the next 6 hours based on the twelve previous. \n",
    "This is a classic auto regression approach like ARIMA. We tried, and got pretty bad results...\n",
    "\n",
    "In this chapter, we try to add information by creating new features, based on the observation of the time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bbaf0144fb05a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## People"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fbf7db3af245",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the data, it seem obvious that the electric power rate is related to the building occupancy.\n",
    "In the previous plot you can easily identify the weekend, the days of the week, and even when occupants start or leave every day.\n",
    "How ? Because you identify at least two behaviour, with high power demand, and with low power demand fluctuations.\n",
    "Let's see if we can create a feature that indicates people presence in the building.\n",
    "\n",
    "To do this, we will use <code>KdeSetpointIdentificator</code> object.\n",
    "To help it find spots, we will smooth the data using a <code>PdGaussianFilter1D</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617cd5ab8214a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:57.792359600Z",
     "start_time": "2024-01-02T08:48:52.255826600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from corrai.transformers import PdSkTransformer, PdGaussianFilter1D\n",
    "from corrai.learning.cluster import KdeSetPointIdentificator, plot_kde_set_point, plot_time_series_kde\n",
    "\n",
    "kde_pipe = make_pipeline(\n",
    "    PdSkTransformer(StandardScaler()),\n",
    "    PdGaussianFilter1D(sigma=4),\n",
    "    KdeSetPointIdentificator(lik_filter=0.5, cluster_tol=0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed880ff50ef8e1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's fit this transformers on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202222c3fc99e797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:58.383267700Z",
     "start_time": "2024-01-02T08:48:57.793967Z"
    }
   },
   "outputs": [],
   "source": [
    "kde_pipe.fit(train_md.get_corrected_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca328b11024a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:59.833723100Z",
     "start_time": "2024-01-02T08:48:58.377815300Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_kde_set_point(train_md.get_corrected_data(), estimator=kde_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad128a1d1ac61d4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is not bad. It seems that the <code>kde_pipe</code> did a good job at identifying nights, weekends and holidays !\n",
    "\n",
    "To create a feature from this transformer, we wrap it into a function.\n",
    "It will use the predict method to create a time series equal to 0 when it identifies near 0 values (nights and weekends), \n",
    "and -1 when it identifies fast changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07c3f5b8169368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:48:59.938077400Z",
     "start_time": "2024-01-02T08:48:59.836067800Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_people(X) -> pd.DataFrame:\n",
    "    X[\"is_people\"] = kde_pipe.fit_predict(X.dropna())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa476ce889b8137b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's add this to the <code>MeasuredDats</code> pipe, to start building the full preprocessing pipe.\n",
    "We also add a <code>StandardScaler</code> as it is good practice in machine learning process (it reduces the \"distance\" between the features and helps for convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc9a4f2da73fa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:00.033403400Z",
     "start_time": "2024-01-02T08:48:59.936570500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "preprocess_pipeline = deepcopy(train_md.get_pipeline())\n",
    "preprocess_pipeline.steps.extend([\n",
    "    (\"Scaler\", PdSkTransformer(StandardScaler())),\n",
    "    (\"People_schedule\", FunctionTransformer(func=is_people))\n",
    "])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccf5ead01afee2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "you can visualize the new feature using a dummy <code>MeasuredDats</code> object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca994d37c2cd9ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:01.378573600Z",
     "start_time": "2024-01-02T08:49:00.030745700Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy_md = MeasuredDats(preprocess_pipeline.fit_transform(train_raw_df))\n",
    "dummy_md.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49786318b82556a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fourier pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be0e4bd3b7a6ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One of the common solutions to help the model learn, consists in adding sine wave at the desired frequency. Here is an example from a [Tensorflow time series tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series):\n",
    "You can see these new time series as additional coordinate for the data points. It will make the measure happening at the same time of the day or at days close to each other.\n",
    "\n",
    "For practical reason, we will \"automate\" the addition of these sine waves time series using transformers, and the <code>MeasureDats</code> pipeline\n",
    "\n",
    "We use a periodgram to show us the energy_signal harmonics\n",
    "\n",
    "_Note that Electricity_index_kWh is no longer kWh but W. Since we transformed it. However the feature name remains. This is not very clean_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cca672f0b7a19f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:05.813494500Z",
     "start_time": "2024-01-02T08:49:01.379108800Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.learning.time_series import plot_periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff42f6d3d1f459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:06.730980600Z",
     "start_time": "2024-01-02T08:49:05.818872800Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_periodogram(preprocess_pipeline.fit_transform(train_raw_df)[\"Electricity_index_kWh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745d858dae25406",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We identify 4 harmonics :\n",
    "- weekly and daily pattern are confirm by the periodgram\n",
    "- we will also use semi-weekly, 6h and 3h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7540bebeeaa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:07.103262Z",
     "start_time": "2024-01-02T08:49:06.917560100Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.transformers import PdAddFourierPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df576981474e1d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:07.343398900Z",
     "start_time": "2024-01-02T08:49:07.098370600Z"
    }
   },
   "outputs": [],
   "source": [
    "def weekday_encoding(X) -> pd.DataFrame:\n",
    "    X[\"is_working_day\"] = X.index.to_series().apply(\n",
    "        lambda X: 1 if X.weekday() < 5 else 0\n",
    "    )\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69661e19785b90c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:07.493330100Z",
     "start_time": "2024-01-02T08:49:07.339346500Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_pipeline.steps.extend([\n",
    "        ('week_day', PdSkTransformer(FunctionTransformer(func=weekday_encoding))),\n",
    "        ('week', PdAddFourierPairs(frequency=1 / (7 * 24 * 3600), feature_prefix=\"week\")),\n",
    "        ('1/2week', PdAddFourierPairs(frequency=1 / (3.5 * 24 * 3600), feature_prefix=\"1/2week\")),\n",
    "        ('day', PdAddFourierPairs(frequency=1 / (1 * 24 * 3600), feature_prefix=\"day\")),\n",
    "        ('6h', PdAddFourierPairs(frequency=1 / (6 * 3600), feature_prefix=\"6h\")),\n",
    "        ('3h', PdAddFourierPairs(frequency=1 / (3 * 3600), feature_prefix=\"3h\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bcb887a30e415",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In a jupyter Notebook, you can easily get a graphical representation of your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb3784bc1c425c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:07.688406800Z",
     "start_time": "2024-01-02T08:49:07.495133800Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b29dd75dcd469",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now lets transform our training and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc5445776dee7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:08.638878400Z",
     "start_time": "2024-01-02T08:49:07.682823300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = preprocess_pipeline.fit_transform(train_raw_df)\n",
    "valid_df = preprocess_pipeline.transform(valid_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a9c884434ee03",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can use the correlation Matrix to see how the new features explains the Electric power rate we are trying to predict.\n",
    "- \"is_people\" feature seem to bring a lot of information\n",
    "- 8.67E-11 frequencies are not very useful.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0206a8f3788e4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:08.825552200Z",
     "start_time": "2024-01-02T08:49:08.638878400Z"
    }
   },
   "outputs": [],
   "source": [
    "abs(train_df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7b46bf4063bd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Time Series Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd4f488670d6d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our objective is to predict the future. We would like to know the energy rate absorbed by the equipments in the next 6h, based on the previous timesteps.\n",
    "To know how many timestep we need to look back, we need to know how the time series is auto correlated.\n",
    "\n",
    "We use the <code>statsmodels plot_pacf</code> to vizualise it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c3596a6dda882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:10.509754600Z",
     "start_time": "2024-01-02T08:49:08.822974800Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(train_df[\"Electricity_index_kWh\"], lags=12 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835399107d40bae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Based one this results, we will use the 34 last time steps (11h) to predict the next 6h\n",
    "\n",
    "To do this, we train Machine learning model using short sequence of the time series.\n",
    "- Features are the last 11h : energy rate, people, sine waves.\n",
    "- The target will be the next 6h energy rate  \n",
    "\n",
    "For example, the timeseries :\n",
    "\n",
    "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "with a <code>sequence_length = 5</code> and <code>sampling_rate = 1</code>  will become:\n",
    "\n",
    "[0, 1, 2, 3, 4]\n",
    "[1, 2, 3, 4, 5]\n",
    "[2, 3, 4, 5, 6]\n",
    "...\n",
    "[6, 7, 8, 9, 10]\n",
    "\n",
    "For a 2D time series (columns are features, index is time step), the output will be a 3D numpy array of shape [_batch size, time steps, dimensionality_] where _dimensionality_ is the number of features [Aurélien Géron Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow]\n",
    "\n",
    "The sampling of the time series is done using the function <code>time_series_sampling</code>\n",
    "\n",
    "We will use <code>shuffle=True</code> to make sure sequences ... are shuffled, so when we split the dataset between training and validation we get sequences spread across the year, and not a single season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40263131b7b316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:10.700816700Z",
     "start_time": "2024-01-02T08:49:10.509257Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.learning.model_selection import time_series_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00512043da4c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:11.592349200Z",
     "start_time": "2024-01-02T08:49:10.702289600Z"
    }
   },
   "outputs": [],
   "source": [
    "train_np = train_df.to_numpy()\n",
    "train_np = train_np.astype(np.float32)\n",
    "\n",
    "n_step_history = 12 * 4 #12h\n",
    "n_step_future = 6 * 4\n",
    "\n",
    "train_sequences = time_series_sampling(\n",
    "    train_np,\n",
    "    sequence_length=n_step_history + n_step_future,\n",
    "    sampling_rate=1,\n",
    "    sequence_stride=4,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588306fb5c08bf6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:13.941862200Z",
     "start_time": "2024-01-02T08:49:11.592846Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy = MeasuredDats(train_df)\n",
    "dummy.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0daa1b4f5851b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:14.362950Z",
     "start_time": "2024-01-02T08:49:13.944320900Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_np = valid_df.to_numpy()\n",
    "valid_np = valid_np.astype(np.float32)\n",
    "\n",
    "valid_sequences = time_series_sampling(\n",
    "    valid_np,\n",
    "    sequence_length=n_step_history + n_step_future,\n",
    "    sampling_rate=1,\n",
    "    sequence_stride=1,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a31e6eebc5bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:14.576933800Z",
     "start_time": "2024-01-02T08:49:14.364534Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy = MeasuredDats(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68051b79929dc04c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:15.699856600Z",
     "start_time": "2024-01-02T08:49:14.577430900Z"
    }
   },
   "outputs": [],
   "source": [
    "dummy.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5577b73fc939bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:15.894865100Z",
     "start_time": "2024-01-02T08:49:15.699360400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf195177b2182106",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The shape of the obtained training array is [20079, 50, 12] corresponding to the number of sequences, the number of time steps and the number of features.\n",
    "\n",
    "The number of sequences is elevated, but we used <code>sampling_rate=1</code>, meaning that there will be a lot of overlaping between the sequences. This may lead to model over fitting, or artificially improve evaluation metrics, as part of the training sample will also be present in validation sample.\n",
    "\n",
    "That is why final performance will be evaluated on the test sample\n",
    "\n",
    "It is now time to isolate the target from the sequences. \n",
    "Using the previous example :\n",
    "[0, 1, 2, 3, 4] \n",
    "[1, 2, 3, 4, 5]\n",
    "[2, 3, 4, 5, 6]\n",
    " … \n",
    "[6, 7, 8, 9, 10]\n",
    "\n",
    "With an history of <code>n_step_history=3</code> and <code>n_step_future=2</code> we get\n",
    "X = [0, 1, 2] \n",
    "    [1, 2, 3]\n",
    "    [2, 3, 4]\n",
    "     … \n",
    "    [6, 7, 8]\n",
    "    \n",
    "Y = [3, 4] \n",
    "    [4, 5]\n",
    "    [5, 6]\n",
    "     … \n",
    "    [9, 10]\n",
    "\n",
    "We also want to split X and Y into two pair of samples X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30659a26d52d1067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:16.090360700Z",
     "start_time": "2024-01-02T08:49:15.896939200Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = (\n",
    "    train_sequences[:, :n_step_history, :],\n",
    "    train_sequences[:, -n_step_future:, 0],\n",
    ")\n",
    "\n",
    "x_valid, y_valid = (\n",
    "    valid_sequences[:, :n_step_history, :],\n",
    "    valid_sequences[:, -n_step_future:, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3858cd1ba1174",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad9219d5421d53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this section, we will train several model on the train_sequence, and observe some error metrics on the validation sample.\n",
    "We won't go in details in the model, we invite you to read A. Geron's books \"Hands on Machine learing ... 2nd editions.\n",
    "All the tested models are implementation of this book suggestions :\n",
    "- Linear combination \n",
    "- Multi layer neural network\n",
    "- Sequential Network with LSTM and GRU cells. Y is reshaped so that the model works \"sequences to sequences\"\n",
    "- Simple implementation of Wavenet : 1D convolution layers stacked\n",
    "\n",
    "The error metrics will be :\n",
    "- Mean square error (MSE)\n",
    "- Symetric Mean Average Percentage Error (SMAPE) \n",
    "\n",
    "All the errors metrics are gathered in a dictionary <code>res_metrics</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89e26a0edf0060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:16.269012Z",
     "start_time": "2024-01-02T08:49:16.091355400Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.learning.time_series import TsDeepNN, DeepRNN, SimplifiedWaveNet\n",
    "from corrai.metrics import last_time_step_mse, last_time_step_smape, smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff003913fd80c770",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:16.456821200Z",
     "start_time": "2024-01-02T08:49:16.266970Z"
    }
   },
   "outputs": [],
   "source": [
    "res_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67631a3575ea077d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:22.203416100Z",
     "start_time": "2024-01-02T08:49:16.458445700Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_linear = TsDeepNN(\n",
    "    metrics=[smape],\n",
    "    patience=200,\n",
    "    max_epoch=20,\n",
    ")\n",
    "ts_linear.fit(x_train, y_train, x_valid, y_valid)\n",
    "res_metrics[\"ts_linear\"] = ts_linear.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fb26357aa2bb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:49:28.087950700Z",
     "start_time": "2024-01-02T08:49:22.202418500Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_rn = TsDeepNN(\n",
    "    hidden_layers_size=3,\n",
    "    metrics=[smape],\n",
    "    patience=200,\n",
    "    max_epoch=20,\n",
    ")\n",
    "simple_rn.fit(x_train, y_train, x_valid, y_valid)\n",
    "res_metrics[\"simple_rn\"] = simple_rn.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817b6fa18170b97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:50:24.330290300Z",
     "start_time": "2024-01-02T08:49:28.087950700Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_seq = DeepRNN(\n",
    "    cells=\"LSTM\",\n",
    "    n_units=40,\n",
    "    hidden_layers_size=1,\n",
    "    reshape_sequence_to_sequence=True,\n",
    "    metrics=[last_time_step_smape, last_time_step_mse],\n",
    "    # optimizer=keras.optimizers.SGD(0.01),\n",
    "    patience=200,\n",
    "    max_epoch=20,\n",
    "    # loss=smape,\n",
    ")\n",
    "lstm_seq.fit(x_train, y_train, x_valid, y_valid)\n",
    "res_metrics[\"lstm_seq\"] = lstm_seq.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c656db27a53f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:51:28.424850700Z",
     "start_time": "2024-01-02T08:50:24.318558800Z"
    }
   },
   "outputs": [],
   "source": [
    "gru_seq = DeepRNN(\n",
    "    cells=\"GRU\",\n",
    "    n_units=40,\n",
    "    hidden_layers_size=1,\n",
    "    reshape_sequence_to_sequence=True,\n",
    "    metrics=[last_time_step_smape, last_time_step_mse],\n",
    "    # optimizer=keras.optimizers.SGD(0.01),\n",
    "    patience=200,\n",
    "    max_epoch=25,\n",
    "    # loss=smape,\n",
    ")\n",
    "gru_seq.fit(x_train, y_train, x_valid, y_valid)\n",
    "res_metrics[\"gru_seq\"] = gru_seq.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e03a9609b10fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:02.666168400Z",
     "start_time": "2024-01-02T08:51:28.422536100Z"
    }
   },
   "outputs": [],
   "source": [
    "wave_net = SimplifiedWaveNet(\n",
    "    convolutional_layers=4,\n",
    "    staked_groups=2,\n",
    "    groups_filters=50,\n",
    "    metrics=[last_time_step_smape, last_time_step_mse],\n",
    "    # optimizer=keras.optimizers.SGD(0.01),\n",
    "    patience=200,\n",
    "    max_epoch=25,\n",
    "    # loss=smape,\n",
    ")\n",
    "wave_net.fit(x_train, y_train, x_valid, y_valid)\n",
    "res_metrics[\"wave_net\"] = wave_net.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504249a70367803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:02.887494400Z",
     "start_time": "2024-01-02T08:52:02.668200300Z"
    }
   },
   "outputs": [],
   "source": [
    "print(res_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efb127c562a3ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's face it. The results are not very good.\n",
    "According to the errors metrics, the most advanced models hardly beat the linear combinations of the features.\n",
    "\n",
    "But as we will see later, predictions using simplified wave net or Sequence to sequence LSTM provide better results for timesteps far in the future.\n",
    "\n",
    "To better visualize the model prediction, we will reshape the sequences into a DataFrame, unscale it to get back Watts, compute metrics timestep per timestep, and plot the results.\n",
    "\n",
    "This time we are going to use the test sample. (part of it, because of a gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12987a7e40f19ce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:03.266461Z",
     "start_time": "2024-01-02T08:52:02.887210800Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = preprocess_pipeline.transform(tests_raw_df).loc[\"2023-10-31\": :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fdd92e8ed44e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:06.921822300Z",
     "start_time": "2024-01-02T08:52:03.268536600Z"
    }
   },
   "outputs": [],
   "source": [
    "from corrai.learning.time_series import sequence_prediction_to_frame\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model_collection = {\n",
    "    \"ts_linear\": ts_linear,\n",
    "    \"simple_rn\": simple_rn, \n",
    "    \"lstm_seq\": lstm_seq,\n",
    "    \"gru_seq\": gru_seq,\n",
    "    \"wave_net\": wave_net}\n",
    "\n",
    "error_df = pd.DataFrame(columns=[i for i in range(n_step_future)], index=list(model_collection.keys()))\n",
    "for mod_name, model in model_collection.items():\n",
    "\n",
    "    predictions = sequence_prediction_to_frame(\n",
    "        model=model, \n",
    "        data=test_df,\n",
    "        target_index=0, \n",
    "        sequence_stride=1,\n",
    "        n_step_history=n_step_history,\n",
    "        n_step_future=n_step_future\n",
    "    )\n",
    "    \n",
    "    unscaled = pd.concat([\n",
    "        preprocess_pipeline.steps[1][1].inverse_transform(predictions[col].to_frame())\n",
    "        for col in predictions\n",
    "     ], axis=1\n",
    "    )\n",
    "    \n",
    "    target_column = unscaled.columns[0]\n",
    "    for col in unscaled.columns[1:]:\n",
    "        # Drop NaN values and calculate RMSE\n",
    "        valid_values = unscaled[[target_column, col]].dropna()\n",
    "        err = mean_absolute_error(valid_values[target_column], valid_values[col])\n",
    "        error_df.loc[mod_name, col] = err\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f7ab584d1de95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:07.730840Z",
     "start_time": "2024-01-02T08:52:06.923901800Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "sns.heatmap(error_df.to_numpy().astype(float), annot=True, cmap='coolwarm', linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd363fe6af3aac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The representation of the Mean Absolute error for each model and one timestep at a time provides more details on the models performances.\n",
    "After the first two time steps, the accuracy of the Linear model and of the de neural network drop dramaticaly.\n",
    "The 3 other models provide similar results, maybe a bit better for the one using LSTM cells\n",
    "\n",
    "We will now plot the prediction against the real data\n",
    "\n",
    "We do it for the LSTM model, but feel free to test other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edc2384b7d8023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:08.939594300Z",
     "start_time": "2024-01-02T08:52:07.731836800Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = sequence_prediction_to_frame(\n",
    "    model=gru_seq, \n",
    "    data=test_df,\n",
    "    target_index=0, \n",
    "    sequence_stride=1,\n",
    "    n_step_history=n_step_history,\n",
    "    n_step_future=n_step_future\n",
    ")\n",
    "\n",
    "unscaled = pd.concat([\n",
    "    preprocess_pipeline.steps[1][1].inverse_transform(predictions[col].to_frame())\n",
    "    for col in predictions\n",
    " ], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257657bf493fcb72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T08:52:10.826956700Z",
     "start_time": "2024-01-02T08:52:08.941053500Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(unscaled, x=unscaled.index, y=unscaled.columns, title=\"Sequence Prediction Plot\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67521881891ff57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fa7f8fcb35eb8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We proposed a full framework from energy meter time series pre-processing, to electric power forecasting.\n",
    "\n",
    "The results are a bit disappointing (encouraging depending on how full you see the glass).\n",
    "\n",
    "- LSTM models seem to be slightly more efficient, but smooth out the time series, and is not able to predict the small variation.\n",
    "- Wave net like model, tries to predict small patterns, but overall, it is a bit less accurate\n",
    " \n",
    "There are a lot of things to do to try to improve these models :\n",
    "- try to modify the architecture, for example add convolutional layers to RNN models\n",
    "- try to modify the sequences splitting (increasing stride, time step history, etc.)\n",
    "- try to tune the hyper-parameters\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
