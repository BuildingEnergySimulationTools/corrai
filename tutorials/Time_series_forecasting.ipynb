{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50880702abcbdcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from corrai.measure import MeasuredDats\n",
    "from copy import deepcopy\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926bd05022e47ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968da07551c7346",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This tutorial guides you through the process of training a Machine Learning (ML) model in order to predict at short term ~6h the electricity consumption of a tertiary building office equipments. \n",
    "\n",
    "The tutorial will follow these big steps:\n",
    "- Load the raw data provided by the Building Energy Monitoring System (BEMS)\n",
    "- Use the Corrai object <code>MeasureDats</code> to pre-process, clean, and visualize the data\n",
    "- Train several ML model, select the most appropriate and tune its hyper parameters\n",
    "- Assess and discuss the model performance\n",
    "- Wrap the model and the processing pipes for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccf80139e72cef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5124961fd4cd0d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data are coming from an electrical energy meter that returns an increasing index in kWh. The timestep is variable between 11 min and 22min. The electric equipments are office equipment, and probably some lab equipments\n",
    "\n",
    "First lets load the csv file containing a single column of data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ccfee681486f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(Path(os.getcwd()) / \"resources/compteur_elec.csv\", index_col=0, sep=';')\n",
    "data_df.index = pd.to_datetime(data_df.index, format='mixed', utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de1bf4c94a359e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets use the <code>DataFrame</code> <code>info()</code> method to get some insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37043a29ba9a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561683e4aee6756",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It looks that data are available from the 2022-12-09 to the 2023-11-22 (in order to get full days).\n",
    "It also looks that among the 22992 entries, none are Nan. This is good news\n",
    "\n",
    "We will use the <code>MeasureDats</code> object to get a visual representation and try to identify the gaps that will be annoying (greater than 3h, where a linear the interpolation may become \"dangerous\").\n",
    "\n",
    "_Note : this is a bit of a misuse of MeasureDats. For a proper introduction to the class, see the corresponding tutorial_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb6cb5bb38eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = MeasuredDats(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbcb174ebb2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.plot_gaps(gaps_timestep=\"3H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420fd6d76fe2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.get_gaps_description(gaps_timedelta='3H')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c806ef98e043c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- A total of 5 gaps greater than 3h minutes were found.\n",
    "- The greater gap is 4 days long\n",
    "\n",
    "This is not bad for nearly a full year of data\n",
    "\n",
    "We can go a bit further and get the date of the gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10cdeb67d6eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.measure import find_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad604027bb2ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_gaps(data_df, timestep='3h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0c25b4d69215",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Given this results, we will probably trash the following periods:\n",
    "- __2023-04-07__ : 16h is to long for interpolation\n",
    "- __2023-05-19 to 2023-05-29__ : we choose to trash a full period. As we will see later, we are going to build sequences to train the ML models. Gaps will introduce errors, so we want to limit there number. It is better to throw away some good days\n",
    "- __2023-10-28 to 2023-10-30__ : the gap span over these two days\n",
    "\n",
    "Moreover, the previous figure showed an anomaly with the energy meter returning a negative value. This shall be removed*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026ee238fde92c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "⚠️ __WARNING__ ⚠️\n",
    "Before going any further, we proppose to split the dataset in 3 parts : __training, validation, testing__.\n",
    "This will avoid to intruduce bias that could artificially improve our models performance.\n",
    "For example, when fitting transformers on the full Dataset, we involontary introduce knowledge on the data we will later try to predict.\n",
    "\n",
    "We proppose the following repartition : __80% training, 10% validation, 10% testing__\n",
    "\n",
    "Moreover we will not shuffle the dataset before splitting it. We are working with time series so the data are coherent chronologically. The training dataset will corespond to the beginning of the year, while the testing will correspond to the end.\n",
    "This could be a major issue if the data had seasonal trend. But we will see later that it is not the case.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc037dee74ef1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = data_df.shape[0]\n",
    "train_raw_df = data_df.iloc[:int(data_size*0.8), :].copy()\n",
    "valid_raw_df = data_df.iloc[int(data_size*0.8):int(data_size*0.9), :].copy()\n",
    "tests_raw_df = data_df.iloc[int(data_size*0.9):, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bae2d36639264f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f82ffcaa66079",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The energy meter data returns a monotonously increasing time series.\n",
    "However we would like to have the electric energy rate [W] absorbed by the equipments.\n",
    "We propose the following transformations to process the time series :\n",
    "- remove negative values\n",
    "- convert the kWh to Joules\n",
    "- apply time gradient\n",
    "- Fill in the small gaps (<=3h) using linear interpolation\n",
    "- Apply Back filling and Front filling operation to make sure no Nan value are present at the beginning and at the and of the time series\n",
    "- Resample the timeseries to get a constant 15 min timestep\n",
    "- The resampling introduced new gaps, when the timestep was greater than 15min. Fill them using linear interpolation again.\n",
    "\n",
    "To easily generate the pipeline and visualise its effect, we use the <code>common_transformations</code> of the <code>MeasureDats</code>. We define a single transformation process called ... <code>process</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3652aac0490c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.measure import Transformer, AGG_METHOD_MAP, AggMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b0b2a835f111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_md = MeasuredDats(train_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b0224710490e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_md.common_trans = {\n",
    "    \"Process\": [\n",
    "        [Transformer.DROP_THRESHOLD, {'lower': 0}],\n",
    "        [Transformer.APPLY_EXPRESSION, {\"expression\": \"X * 1000 * 3600\"}],\n",
    "        [Transformer.TIME_GRADIENT, {}],\n",
    "        [Transformer.INTERPOLATE, {\"method\": \"linear\"}],\n",
    "        [Transformer.BFILL, {}],\n",
    "        [Transformer.FFILL, {}],\n",
    "        [Transformer.RESAMPLE, {\"rule\": \"20T\", \"method\": AGG_METHOD_MAP[AggMethod.MEAN]}],\n",
    "        [Transformer.INTERPOLATE, {\"method\": \"linear\"}],\n",
    "    ]\n",
    "}\n",
    "\n",
    "train_md.transformers_list = [\"Process\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf06afe4199216d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets see the effect of the pipe !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61686d231e0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_md.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a527e903a2389b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It really looks good ! We can make the following observation :\n",
    "- There is a weekly pattern with fewer consumption during the weekends\n",
    "- There is a daily pattern, the energy rate starts to increase around 6 o clock and drops around 17h\n",
    "- For an unknown reason, there seem to be a pattern of period ~2h. It could be chiller, for a server local for exemple\n",
    "- It's not very clear, but the energy rate demand seem to increase slightly during the cold months\n",
    "- There seem to be a holiday period from the 25 December to the 2nd of january\n",
    "\n",
    "We can also see the gaps in the data. For example, if you try to zoom in on the second half of the month of May, you should see straight lines corresponding the linear interpolation. Lets remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d09e36c57766a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_raw_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cca49bfa1cc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_list_of_day(df, list_of_day):\n",
    "    \"\"\"\n",
    "    Drops rows from a pandas DataFrame based on a list of specific days.\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame from which rows will be dropped.\n",
    "    - list_of_day (list): A list of dates in string format ('YYYY-MM-DD') representing the days to be dropped.\n",
    "    \"\"\"\n",
    "    days_to_drop_dt = pd.to_datetime(list_of_day, errors='coerce')\n",
    "    return df[~df.index.strftime('%Y-%m-%d').isin(days_to_drop_dt.strftime('%Y-%m-%d'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03fc55118ea5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of full days to drop\n",
    "days_to_drop_train = [\n",
    "    \"2023-04-07\",\n",
    "    \"2023-05-19\",\n",
    "    \"2023-05-20\",\n",
    "    \"2023-05-21\",\n",
    "    \"2023-05-22\",\n",
    "    \"2023-05-23\",\n",
    "    \"2023-05-24\",\n",
    "    \"2023-05-25\",\n",
    "    \"2023-05-26\",\n",
    "    \"2023-05-27\",\n",
    "    \"2023-05-28\",\n",
    "    \"2023-05-29\",\n",
    "]\n",
    "days_to_drop_test = [\n",
    "    \"2023-10-28\",\n",
    "    \"2023-10-29\",\n",
    "    \"2023-10-30\",\n",
    "]\n",
    "\n",
    "train_raw_df = drop_list_of_day(train_raw_df, days_to_drop_train)\n",
    "tests_raw_df = drop_list_of_day(tests_raw_df, days_to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16408518892d657",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Add features to the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bbaf0144fb05a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## People"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fbf7db3af245",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the data, it seem obvious that the electric power rate is related to the building occupancy.\n",
    "In the previous plot you can easily identify the weekend, the days of the week, and even when occupants start or leave every day.\n",
    "How ? Because you identify at least two behaviour, with high power demand, and with low power demand fluctuations.\n",
    "Let's see if we can create a feature that indicates people presence in the building.\n",
    "\n",
    "To do this, we will use <code>KdeSetpointIdentificator</code> object.\n",
    "To help it find spots, we will smooth the data using a <code>PdGaussianFilter1D</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617cd5ab8214a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from corrai.transformers import PdSkTransformer, PdGaussianFilter1D\n",
    "from corrai.learning.cluster import KdeSetPointIdentificator, plot_kde_set_point, plot_time_series_kde\n",
    "\n",
    "kde_pipe = make_pipeline(\n",
    "    PdSkTransformer(StandardScaler()),\n",
    "    PdGaussianFilter1D(sigma=3),\n",
    "    KdeSetPointIdentificator(lik_filter=0.5, cluster_tol=0.2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed880ff50ef8e1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's fit this transformers on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202222c3fc99e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_pipe.fit(train_md.get_corrected_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca328b11024a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kde_set_point(train_md.get_corrected_data(), estimator=kde_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad128a1d1ac61d4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is not bad. It seems that the kde_pipe did a good job at identifying nights, weekends and holidays !\n",
    "\n",
    "To create a feature from this transformer, we wrap it into a function.\n",
    "It will use the predict method to create a time series equal to 0 when it identifies near 0 values (nights and weekends), \n",
    "and -1 when it identify fast changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07c3f5b8169368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_people(X) -> pd.DataFrame:\n",
    "    X[\"is_people\"] = kde_pipe.fit_predict(X.dropna())\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa476ce889b8137b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's add this to the <code>MeasuredDats</code> pipe, to start building the full preprocessing pipe.\n",
    "We also add a <code>StandardScaler</code> as it is good practice in machine learning process (it reduces the \"distance\" between the features and helps for convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc9a4f2da73fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = deepcopy(train_md.get_pipeline())\n",
    "preprocess_pipeline.steps.extend([\n",
    "    PdSkTransformer(StandardScaler()),\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be0e4bd3b7a6ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We could use the time series, split it into sequences 18h for example, and try to predict the next 6 hours based on the twelve previous. \n",
    "We tried, and got pretty bad results...\n",
    "It seems logic as only 12 hours is not enough to capture the frequencies mentioned earlier (period of 24h and 7 days).\n",
    "We could increase the length of the sequence, but we would reduce the number of available sequence, or create to many overlaps which could cause training problems.\n",
    "\n",
    "One of the common solutions to help the model learn, consists in adding sine wave at the desired frequency. Here is an example : https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "You can see these new time series as additional coordinate for the data points. It will make the measure happening at the same time of the day or at the same day of the week closer.\n",
    "\n",
    "For practical reason, we will \"automate\" the addition of these sine waves time series using transformers, and the <code>MeasureDats</code> pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7540bebeeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.transformers import PdAddFourierPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69661e19785b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = deepcopy(my_data.get_pipeline())\n",
    "preprocess_pipeline.steps.extend([\n",
    "    (\"Sine_7D\", PdAddFourierPairs(frequency=1 / (7 * 24 * 3600))),\n",
    "    (\"Sine_3.5\", PdAddFourierPairs(frequency=1 / (3.5 * 24 * 3600))),\n",
    "    (\"Sine_24h\", PdAddFourierPairs(frequency=1 / (1 * 24 * 3600))),\n",
    "    (\"Sine_6h\", PdAddFourierPairs(frequency=1 / (6 * 3600))),\n",
    "    (\"Sine_3h\", PdAddFourierPairs(frequency=1 / (3 * 3600)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bcb887a30e415",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In a jupyter Notebook, you can easily get a graphical representation of your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb3784bc1c425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21b1e8d8d04f7a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's add a columns to the original DataFrame. The values are 0 when there is holidays, and \n",
    "1 when it's a day at work. We do not add this using a transformer in a pipeline, since it depends on the occupant behaviour (or their boss's). It is more a \"measure\" or at least a schedule that must come from the BEMS system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff5f68c9ade87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"Holidays\"] = np.ones(data_df.shape[0])\n",
    "data_df.loc[\"2022-12-24\":\"2023-01-01\", \"Holidays\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625093466993205",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before doing further manipulation, we are going to isolate a part of the dataset (let's say ~10% or ~5 weeks).\n",
    "This is called the test sample. We will not even look at it during the ML training process.\n",
    "It will only be used to evaluate the model performance\n",
    "\n",
    "Although there might be a seasonal effect that it is not taken into account, we choose to select the las 2254 timestep as the test sample. It is better to have a coherent sequence and not random fragments of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b3bb83c8c09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_df = data_df.iloc[:data_df.shape[0] - 2254, :]\n",
    "test_df = data_df.iloc[data_df.shape[0] - 2254:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cd713216ce682",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before moving on to sampling and ML model fitting, we are going to scale the features. ML models don't like large distance between points, it is good practice to scale your data. \n",
    "Fortunately <code>PdSkTransformer</code> shall make scaling and unscaling easy.\n",
    "The scaler is append to the <code>preprocess_pipeline</code> pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebdda610d29f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from corrai.transformers import PdSkTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec8d4c0d5de2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.steps.append(\n",
    "    ('Standard_scaler', PdSkTransformer(StandardScaler()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43f2cbd41414ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lastly we push the training / validation data through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb22975897480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_val_df = preprocess_pipeline.fit_transform(train_valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec457f458c881f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_val_df.loc[scaled_train_val_df[\"Holidays\"] > 1, \"Holidays\"] = 1\n",
    "scaled_train_val_df.loc[scaled_train_val_df[\"Holidays\"] < 0, \"Holidays\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7b46bf4063bd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Time Series Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835399107d40bae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Our objective is to predict the future. We would like to know the energy rate absorbed by the equipments in the next 6h, based on the last 12h.\n",
    "\n",
    "To do this, we train Machine learning model using short sequence of the time series.\n",
    "- Features are the last 12h : energy rate, 2h, 24h, 7D sine waves and the holidays.\n",
    "- The target will be the next 6h energy rate  \n",
    "\n",
    "For example, the timeseries :\n",
    "\n",
    "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "with a <code>sequence_length = 5</code> and <code>sampling_rate = 1</code>  will become:\n",
    "\n",
    "[0, 1, 2, 3, 4]\n",
    "[1, 2, 3, 4, 5]\n",
    "[2, 3, 4, 5, 6]\n",
    "...\n",
    "[6, 7, 8, 9, 10]\n",
    "\n",
    "For a 2D time series (columns are features, index is time step), the output will be a 3D numpy array of shape [_batch size, time steps, dimensionality_] where _dimensionality_ is the number of features [Aurélien Géron Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow]\n",
    "\n",
    "The sampling of the time series is done using the function <code>time_series_sampling</code>\n",
    "\n",
    "We will use <code>shuffle=True</code> to make sure sequences ... are shuffled, so when we split the dataset between training and validation we get sequences spread across the year, and not a single season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40263131b7b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.learning.model_selection import time_series_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00512043da4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_sequences = time_series_sampling(\n",
    "    scaled_train_val_df,\n",
    "    sequence_length=4 * (48 + 6), #12h history + 6h in the future at a 15min timmstep,\n",
    "    sampling_rate=1,\n",
    "    sequence_stride=4 * (48 + 6) / 2,\n",
    "    shuffle=True,\n",
    "    seed=42 # Make sure the behaviour can be repeated in the notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5acdf6a1ea20abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf195177b2182106",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The shape of the obtained array is [29993, 72, 5] corresponding to the number of sequences, the number of time steps and the number of features.\n",
    "\n",
    "The number of sequences is elevated, but we used <code>sampling_rate=1</code>, meaning that there will be a lot of overlaping between the sequences. This may lead to model over fitting, or artificially improve evaluation metrics, as part of the training sample will also be present in validation sample.\n",
    "\n",
    "That is why final performance will be evaluated on the test sample\n",
    "\n",
    "It is now time to isolate the target from the sequences. \n",
    "Using the previous example :\n",
    "[0, 1, 2, 3, 4] \n",
    "[1, 2, 3, 4, 5]\n",
    "[2, 3, 4, 5, 6]\n",
    " … \n",
    "[6, 7, 8, 9, 10]\n",
    "\n",
    "With an history of <code>n_step_history=3</code> and <code>n_step_future=2</code> we get\n",
    "X = [0, 1, 2] \n",
    "    [1, 2, 3]\n",
    "    [2, 3, 4]\n",
    "     … \n",
    "    [6, 7, 8]\n",
    "    \n",
    "Y = [3, 4] \n",
    "    [4, 5]\n",
    "    [5, 6]\n",
    "     … \n",
    "    [9, 10]\n",
    "\n",
    "We also want to split X and Y into two pair of samples X_train, y_train, X_valid, y_valid\n",
    "\n",
    "To do this we will use the <code>sequences_train_test_split</code> function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30659a26d52d1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.learning.model_selection import sequences_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1c3eac6b79478",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = sequences_train_test_split(\n",
    "    train_valid_sequences,\n",
    "    targets_index=0,\n",
    "    n_steps_history=48 * 4,\n",
    "    n_steps_future=6 * 4,\n",
    "    train_size=0.80,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3858cd1ba1174",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89e26a0edf0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.learning.time_series import TsDeepNN, DeepRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae22d66adceacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_linear = TsDeepNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2c5d1ff9db879",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_linear.fit(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009df186610ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe739a50303b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R² = {ts_linear.score(x_valid, y_valid)} \\n\"\n",
    "      f\"MSE = {mean_squared_error(ts_linear.predict(x_valid), y_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f7ab584d1de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.learning.time_series import plot_sequence_forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb8023537bb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sequence_forcast(x_valid, y_valid, model=ts_linear, batch_nb=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12b1c9ad895911",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91946c3da69a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_rnn = TsDeepNN(hidden_layers_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a147ace8a29070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_rnn.fit(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179af833ff2f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R² = {deep_rnn.score(x_valid, y_valid)} \\n\"\n",
    "      f\"MSE = {mean_squared_error(deep_rnn.predict(x_valid), y_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba26ce5573cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sequence_forcast(x_valid, y_valid, model=deep_rnn, batch_nb=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f068fbda4db99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.metrics import last_time_step_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8813de73ea8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = DeepRNN(\n",
    "    cells='LSTM',\n",
    "    hidden_layers_size=1,\n",
    "    reshape_sequence_to_sequence=True,\n",
    "    n_units=40,\n",
    "    metrics=last_time_step_rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8dcda68c5e66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417e7e634878508",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335aaf230859be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sequence_forcast(x_valid, y_valid, model=rnn, batch_nb=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aadf7f460aa4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corrai.learning.time_series import SimplifiedWaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909616f8bec906f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_net = SimplifiedWaveNet(\n",
    "    convolutional_layers=3,\n",
    "    staked_groups=2,\n",
    "    groups_filters=100,\n",
    "    metrics=[last_time_step_rmse])\n",
    "wave_net.fit(x_train, y_train, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58f2ca45f81164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_sequence_forcast(X, y_ref, model, X_target_index=0, batch_nb=0):\n",
    "    \"\"\"\n",
    "    Plot the historical, reference, and predicted values for a given\n",
    "    batch number in X using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    :param X: np.ndarray 3D array of shape [batch_size, time_step, dimension]\n",
    "    Input sequences.\n",
    "    :param y_ref: np.ndarray of shape [batch_size, time_step].\n",
    "    :param model: keras.Model The trained model for making predictions.\n",
    "    :param X_target_index: The index of 3rd dimension in X that contains\n",
    "        historical values.\n",
    "    :param batch_nb: int Batch number to visualize. Default is 0.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    predictions = predictions[batch_nb, :]\n",
    "    y_ref_to_plot = y_ref[batch_nb, :]\n",
    "    x_to_plot = X[batch_nb, :, X_target_index]\n",
    "\n",
    "    # Create traces for historic, new, and predicted values\n",
    "    historic_trace = go.Scatter(\n",
    "        x=list(range(len(x_to_plot))),\n",
    "        y=x_to_plot,\n",
    "        mode='markers+lines',\n",
    "        name='Historic Values'\n",
    "    )\n",
    "\n",
    "    new_trace = go.Scatter(\n",
    "        x=list(range(len(x_to_plot), len(x_to_plot) + len(y_ref_to_plot))),\n",
    "        y=y_ref_to_plot,\n",
    "        mode='markers+lines',\n",
    "        name='New Values'\n",
    "    )\n",
    "\n",
    "    predicted_trace = go.Scatter(\n",
    "        x=list(range(len(x_to_plot), len(x_to_plot) + len(y_ref_to_plot))),\n",
    "        y=predictions,\n",
    "        mode='markers+lines',\n",
    "        name='Predicted Values'\n",
    "    )\n",
    "\n",
    "    # Create layout\n",
    "    layout = go.Layout(\n",
    "        title='Actual and Predicted Values Over Time',\n",
    "        xaxis=dict(title='Number of Timesteps'),\n",
    "        yaxis=dict(title='x(t)'),\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[historic_trace, new_trace, predicted_trace], layout=layout)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5207af6629879",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sequence_forcast(x_valid, y_valid, model=wave_net, batch_nb=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4bb11bbe8670b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d5939f0e5640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed = preprocess_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59253f0ed953ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed[\"Holidays\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0957d58bb1c71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae591e2bc3ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = time_series_sampling(\n",
    "    test_transformed,\n",
    "    sequence_length=4 * (48 + 6),  #12h history + 6h in the future at a 15min timestep,\n",
    "    sampling_rate=1,\n",
    "    shuffle=False,\n",
    "    seed=42  # Make sure the behaviour can be repeated in the notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcbe05569008d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, x_empty, y_test, y_empty = sequences_train_test_split(\n",
    "    test_sequences,\n",
    "    targets_index=0,\n",
    "    n_steps_history=48 * 4,\n",
    "    n_steps_future=6 * 4,\n",
    "    train_size=0.1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6c9db05263369",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22727ad383c8084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_net.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0869d2bd42b3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sequence_forcast(x_test, y_test, model=deep_rnn, batch_nb=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9710c8b2a2eda30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
